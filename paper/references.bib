@article{anitescu2000degenerate,
  title={Degenerate nonlinear programming with a quadratic growth condition},
  author={Anitescu, Mihai},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={4},
  pages={1116--1135},
  year={2000},
  publisher={SIAM}
}

@article{bonnans1993second,
  title={Second-order sufficiency and quadratic growth for nonisolated minima},
  author={Bonnans, Joseph Fr{\'e}d{\'e}ric and Ioffe, Alexander},
  journal={Mathematics of Operations Research},
  volume={20},
  number={4},
  pages={801--817},
  year={1995},
  publisher={INFORMS}
}


@article{ioffe1994sensitivity,
  title={On sensitivity analysis of nonlinear programs in Banach spaces: the approach via composite unconstrained optimization},
  author={Ioffe, Alexander},
  journal={SIAM Journal on Optimization},
  volume={4},
  number={1},
  pages={1--43},
  year={1994},
  publisher={SIAM}
}
@Book{Rock70,
  author    = {Rockafellar, Ralph T.},
  year      = {1970},
  title     = {Convex Analysis},
  publisher = {Princeton University Press.},
}
@incollection{NIPS2015_5718,
	Author = {Elad Hazan and Kfir Levy and Shai Shalev-Shwartz},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Date-Modified = {2020-09-03 11:52:16 -0400},
	Editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	Pages = {1594--1602},
	Publisher = {Curran Associates, Inc.},
	Title = {Beyond Convexity: Stochastic Quasi-Convex Optimization},
	Year = {2015}
}
	
@article{hardt2016gradient,
	Author = {Moritz Hardt and Tengyu Ma and Benjamin Recht},
	Date-Modified = {2020-08-19 12:19:49 +0200},
	Journal = {Journal of Machine Learning Research},
	Title = {Gradient Descent Learns Linear Dynamical Systems},
	Volume = {19},
	Year = {2018}}
	
@techreport{zhang2013gradient,
	Author = {Hui Zhang and Wotao Yin},
	Date-Modified = {2020-08-19 13:32:10 +0200},
	Institution = {UCLA},
	Title = {Gradient methods for convex minimization: better rates under weaker conditions},
	Type = {CAM Report},
	Year = {2013}}
	
@article{luo_error_1993,
	Abstract = {We survey and extend a general approach to analyzing the convergence and the rate of convergence of feasible descent methods that does not require any nondegeneracy assumption on the problem. This approach is based on a certain error bound for estimating the distance to the solution set and is applicable to a broad class of methods.},
	Author = {Zhi-Quan Luo and Paul Tseng},
	Date-Modified = {2020-09-03 11:54:37 -0400},
	Journal = {Annals of Operations Research},
	Number = {1},
	Pages = {157--178},
	Title = {Error bounds and convergence analysis of feasible descent methods: a general approach},
	Volume = {46},
	Year = {1993}}

@article{polyak_gradient_1963,
	Abstract = {Let tf(t) be a functional defined in the (real) Hubert space H. The problem consists in finding its minimum value tff∗ = inf tf(x) and some minimum point x∗ (if such exists).},
	Author = {Boris T. Polyak},
	Date-Modified = {2020-09-03 11:58:07 -0400},
	Journal = {USSR Computational Mathematics and Mathematical Physics},
	Number = {4},
	Pages = {864 -- 878},
	Title = {Gradient methods for the minimisation of functionals},
	Volume = {3},
	Year = {1963},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0041555363903823},
	Bdsk-Url-2 = {https://doi.org/10.1016/0041-5553(63)90382-3}}
	
@article{kurdyka1998gradients,
	Author = {Krzysztof Kurdyka},
	Date-Modified = {2020-09-03 11:53:16 -0400},
	Journal = {Annales de l'institut Fourier},
	Pages = {769--783},
	Title = {On gradients of functions definable in o-minimal structures},
	Volume = {48},
	Year = {1998}}
	
@article{liu2014asynchronous,
	Author = {Liu, Ji and Wright, Stephen J.},
	Date-Modified = {2020-08-19 13:12:52 +0200},
	Journal = {SIAM Journal on Optimization},
	Number = {1},
	Pages = {351---376},
	Title = {Asynchronous Stochastic Coordinate Descent: Parallelism and Convergence Properties},
	Volume = {25},
	Year = {2015}}
	
@article{gong2014linear,
	Author = {Pinghua Gong and Jieping Ye},
	Date-Modified = {2020-08-19 12:16:10 +0200},
	Journal = {arXiv:1406.1102},
	Title = {Linear Convergence of Variance-Reduced Stochastic Gradient without Strong Convexity},
	Year = {2014}}
	
@inproceedings{guille2021study,
  title={A Study of Condition Numbers for First-Order Optimization},
  author={Guille-Escuret, Charles and Goujaud, Baptiste and Girotti, Manuela and Mitliagkas, Ioannis},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1261--1269},
  year={2021}
}

@misc{cui2017quadratic,
      title={Quadratic growth conditions for convex matrix optimization problems associated with spectral functions}, 
      author={Ying Cui and Chao Ding and Xinyuan Zhao},
      year={2017},
      eprint={1702.03262},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{zhang2017restricted,
  title={The restricted strong convexity revisited: analysis of equivalence to error bound and quadratic growth},
  author={Zhang, Hui},
  journal={Optimization Letters},
  volume={11},
  number={4},
  pages={817--833},
  year={2017},
  publisher={Springer}
}

@article{drusvyatskiy2016error,
  title={Error bounds, quadratic growth, and linear convergence of proximal methods},
  author={Drusvyatskiy, Dmitriy and Lewis, Adrian S},
  journal={Mathematics of Operations Research},
  volume={43},
  number={3},
  pages={919--948},
  year={2018},
  publisher={INFORMS}
}
@article{necoara2016linear,
  title={Linear convergence of first order methods for non-strongly convex optimization},
  author={Necoara, Ion and Nesterov, Yurii and Glineur, Francois},
  journal={Mathematical Programming},
  volume={175},
  number={1},
  pages={69--107},
  year={2019},
  publisher={Springer}
}


@article{drusvyatskiy2015quadratic,
  title={Quadratic growth and critical point stability of semi-algebraic functions},
  author={Drusvyatskiy, Dmitriy and Ioffe, Alexander D.},
  journal={Mathematical Programming},
  volume={153},
  number={2},
  pages={635--653},
  year={2015},
  publisher={Springer}
}

@misc{zhang2017linear,
      title={Linear Convergence of the Proximal Incremental Aggregated Gradient Method under Quadratic Growth Condition}, 
      author={Hui Zhang},
      year={2017},
      eprint={1702.08166},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{zhang2020,
   title={Global complexity analysis of inexact successive quadratic approximation methods for regularized optimization under mild assumptions},
   volume={78},
   number={1},
   journal={Journal of Global Optimization},
   publisher={Springer Science and Business Media LLC},
   author={Peng, Wei and Zhang, Hui and Zhang, Xiaoya and Cheng, Lizhi},
   year={2020},
   pages={69–89}
}

@article{gower2020sgd,
      Title={SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation}, 
      Author={Robert M. Gower and Othmane Sebbouh and Nicolas Loizou},
      Journal = {arXiv:2006.1031},
      year={2020}}
      
@article{agarwal2011fast,
	Author = {Alekh Agarwal and Sahand N. Negahban and Martin J. Wainwright},
	Date-Modified = {2020-08-19 12:09:22 +0200},
	Journal = {Ann. Statist.},
	Number = {5},
	Pages = {2452--2482},
	Title = {Fast global convergence of gradient methods for high-dimensional statistical recovery},
	Volume = {40},
	Year = {2012}}

@article{bauschke2017descent,
  title={A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications},
  author={Bauschke, Heinz H. and Bolte, J{\'e}r{\^o}me and Teboulle, Marc},
  journal={Mathematics of Operations Research},
  volume={42},
  number={2},
  pages={330--348},
  year={2017},
  publisher={Informs}
}

@article{lu2016relativelysmooth,
	Author = {Lu, Haihao and Freund, Robert M.  and Nesterov, Yurii},
	Date-Modified = {2020-08-19 13:15:25 +0200},
	Journal = {SIAM Journal on Optimization},
	Number = {1},
	Pages = {333--354},
	Title = {Relatively-Smooth Convex Optimization by First-Order Methods, and Applications},
	Volume = {28},
	Year = {2018}}
	
@article{dragomir2021optimal,
  title={Optimal complexity and certification of Bregman first-order methods},
  author={Dragomir, Radu-Alexandru and Taylor, Adrien B. and d’Aspremont, Alexandre and Bolte, J{\'e}r{\^o}me},
  journal={Mathematical Programming},
  pages={1--43},
  year={2021},
  publisher={Springer}
}

	
@article{zhou2019simple,
	Author = {Yi Zhou and Yingbin Liang and Lixin Shen},
	Date-Modified = {2020-09-03 11:59:48 -0400},
	Journal = {Computational Optimization and Applications},
	Number = {3},
	Pages = {903--912},
	Publisher = {Springer},
	Title = {A simple convergence analysis of Bregman proximal gradient algorithm},
	Volume = {73},
	Year = {2019}}
	
@misc{chieu2021quadratic,
      title={Quadratic Growth and Strong Metric Subregularity of the Subdifferential for a Class of Non-prox-regular Functions}, 
      author={Nguyen Huy Chieu and Nguyen Thi Quynh Trang and Ha Anh Tuan},
      year={2021},
      eprint={2109.03399},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{taylor2017smooth,
  title={Smooth strongly convex interpolation and exact worst-case performance of first-order methods},
  author={Taylor, Adrien B. and Hendrickx, Julien M. and Glineur, Fran{\c{c}}ois},
  journal={Mathematical Programming},
  volume={161},
  number={1-2},
  pages={307--345},
  year={2017},
  publisher={Springer}
}

@article{taylor2017exact,
  title={Exact worst-case performance of first-order methods for composite convex optimization},
  author={Taylor, Adrien B. and Hendrickx, Julien M. and Glineur, Fran{\c{c}}ois},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={3},
  pages={1283--1313},
  year={2017},
  publisher={SIAM}
}

@article{drori2014performance,
  title={Performance of first-order methods for smooth convex minimization: a novel approach},
  author={Drori, Yoel and Teboulle, Marc},
  journal={Mathematical Programming},
  volume={145},
  number={1},
  pages={451--482},
  year={2014},
  publisher={Springer}
}

@phdthesis{drori2014contributions,
  title={Contributions to the Complexity Analysis of Optimization Algorithms},
  author={Drori, Yoel},
  year={2014},
  school={Tel-Aviv University}
}

@article{kim2016optimized,
  title={Optimized first-order methods for smooth convex minimization},
  author={Kim, Donghwan and Fessler, Jeffrey A},
  journal={Mathematical programming},
  volume={159},
  number={1},
  pages={81--107},
  year={2016},
  publisher={Springer}
}

@masterthesis{daccacheperformance,
  title={Performance estimation of the gradient method with fixed arbitrary step sizes},
  author={Daccache, Antoine},
  year={2019},
  school={UCLouvain}
}

@article{drori2020efficient,
  title={Efficient first-order methods for convex minimization: a constructive approach},
  author={Drori, Yoel and Taylor, Adrien B.},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={183--220},
  year={2020},
  publisher={Springer}
}

@article{bolte2017error,
  title={From error bounds to the complexity of first-order descent methods for convex functions},
  author={Bolte, J{\'e}r{\^o}me and Nguyen, Trong Phong and Peypouquet, Juan and Suter, Bruce W.},
  journal={Mathematical Programming},
  volume={165},
  number={2},
  pages={471--507},
  year={2017},
  publisher={Springer}
}

@inproceedings{taylor2017performance,
  title={{Performance estimation toolbox (PESTO): automated worst-case analysis of first-order optimization methods}},
  author={Taylor, Adrien B. and Hendrickx, Julien M. and Glineur, Fran{\c{c}}ois},
  booktitle={56th Annual Conference on Decision and Control (CDC)},
  pages={1278--1283},
  year={2017}
}

@article{goujaud2022pepit,
  title={{PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python}},
  author={Goujaud, Baptiste and Moucer, C{\'e}line and Glineur, Fran{\c{c}}ois and Hendrickx, Julien and Taylor, Adrien and Dieuleveut, Aymeric},
  journal={preprint arXiv:2201.04040},
  year={2022}
}

@article{flanders1950numerical,
  title={Numerical determination of fundamental modes},
  author={Flanders, Donald A. and Shortley, George},
  journal={Journal of Applied Physics},
  volume={21},
  number={12},
  pages={1326--1332},
  year={1950},
  publisher={American Institute of Physics}
}

@article{lanczos1952solution,
  title={Solution of Systems of Linear Equations by},
  author={Lanczos, Cornelius},
  journal={Journal of research of the National Bureau of Standards},
  volume={49},
  number={1},
  pages={33},
  year={1952},
  publisher={The Bureau}
}

@article{young1953richardson,
  title={On Richardson's method for solving linear systems with positive definite matrices},
  author={Young, David},
  journal={Journal of Mathematics and Physics},
  volume={32},
  number={1-4},
  pages={243--255},
  year={1953},
  publisher={Wiley Online Library}
}

@article{lessard2016analysis,
  title={Analysis and design of optimization methods via integral quadratic constraints},
  author={Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={1},
  pages={57--95},
  year={2016},
  publisher={SIAM}
}


@article{azagra2017extension,
  title={An Extension Theorem for convex functions of class C1, 1 on Hilbert spaces},
  author={Azagra, Daniel and Mudarra, Carlos},
  journal={Journal of Mathematical Analysis and Applications},
  volume={446},
  number={2},
  pages={1167--1182},
  year={2017},
  publisher={Elsevier}
}

@article{lambert2004finite,
  title={Finite convex integration},
  author={Lambert, Delphine and Crouzeix, Jean-Pierre and Nguyen, V. Hien and Strodiot, Jean-Jacques},
  journal={Journal of Convex Analysis},
  volume={11},
  number={1},
  pages={131--146},
  year={2004},
  publisher={Heldermann Verlag LANGER GRABEN 17, 32657 LEMGO, GERMANY}
}

@inproceedings{ghadimi2014global,
  title={Global convergence of the heavy-ball method for convex optimization},
  author={Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={2015 European control conference (ECC)},
  pages={310--315},
  year={2015}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T. and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@Article{nesterov1983method,
  author        = {Nesterov, Yurii},
  year          = {1983},
  journal       = {Soviet Mathematics Doklady},
  title         = {A method of solving a convex programming problem with convergence rate ${O}(1/k^2)$},
  number        = {2},
  pages         = {372-376},
  volume        = {27},
  date-modified = {2005-11-17 10:56:23 -0500},
}


@article{bubeck2014convex,
  title={Convex Optimization: Algorithms and Complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@techreport{ruppert1988efficient,
  title={Efficient estimations from a slowly convergent Robbins-Monro process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}


@article{nemirovskii1985optimal,
  title={Optimal methods of smooth convex minimization},
  author={Nemirovskii, Arkadi S. and Nesterov, Yurii},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={25},
  number={2},
  pages={21--30},
  year={1985},
  publisher={Elsevier}
}

@article{nesterov2013gradient,
  title={Gradient methods for minimizing composite functions},
  author={Nesterov, Yurii},
  journal={Mathematical programming},
  volume={140},
  number={1},
  pages={125--161},
  year={2013},
  publisher={Springer}
}

@article{iouditski2014primal,
  title={Primal-dual subgradient methods for minimizing uniformly convex functions},
  author={Iouditski, Anatoli and Nesterov, Yuri},
  journal={preprint arXiv:1401.1792},
  year={2014}
}

%%%% REFERENCES ADRIEN (already cleaned)
@article{Book:NemirovskyYudin,
	title={Problem Complexity and Method Efficiency in Optimization.},
	year={1983},
	author={Nemirovskii, Arkadi S. and Yudin, David B.},
	journal={Willey-Interscience, New York}
}
@book{Book:polyak1987,
  title={Introduction to optimization},
  author={Polyak, Boris T.},
  year={1987},
  publisher={Optimization Software New York}
}

@Book{Nest03a,
  author        = {Nesterov, Yurii},
  year          = {2003},
  title         = {Introductory Lectures on Convex Optimization},
  publisher     = {Springer},
  date-added    = {2006-04-13 14:25:50 -0400},
  date-modified = {2006-04-18 20:02:34 -0400},
}

@inproceedings{bottou2007tradeoffs,
  title={The tradeoffs of large scale learning},
  author={Bottou, L{\'e}on and Bousquet, Olivier},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2007}
}

@article{hanzely2018accelerated,
  title={Accelerated Bregman proximal gradient methods for relatively smooth convex optimization},
  author={Hanzely, Filip and Richtarik, Peter and Xiao, Lin},
  journal={Computational Optimization and Applications},
  volume={79},
  number={2},
  pages={405--440},
  year={2021},
  publisher={Springer}
}

@misc{nemirovskinotes1995,
  title={Information-based complexity of convex programming},
  author={Nemirovskii, Arkadi S.},
  institution = {Technion},
  howpublished = {Lecture notes},
  year={1994}
}

@article{guille2022gradient,
  title={Gradient Descent Is Optimal Under Lower Restricted Secant Inequality And Upper Error Bound},
  author={Guille-Escuret, Charles and Goujaud, Baptiste and Ibrahim, Adam and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:2203.00342},
  year={2022}
}
@article{abbaszadehpeivasti2022conditions,
  title={Conditions for linear convergence of the gradient method for non-convex optimization},
  author={Abbaszadehpeivasti, Hadi and de Klerk, Etienne and Zamani, Moslem},
  journal={arXiv preprint arXiv:2204.00647},
  year={2022}
}
